{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jBDe1295CJY"
   },
   "source": [
    "Before to begin if you are planning to train a model:\n",
    "\n",
    "(1) set the runtime with a TPU or GPU \n",
    "\n",
    "(2) Make sure a folder tmp_model_dir is not in the working directory from a previous training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFA7poG-88jK",
    "outputId": "85b8caa7-a12b-4817-d6f6-eec5eff3bc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  2 08:29:21 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Print current hardware information (variable in Colab) \n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qL1nvR-8QJS6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title\n",
    "import os\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IspsBhcCp9h6"
   },
   "source": [
    "Read from my Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jlkN4_368tvu",
    "outputId": "30781f52-8186-4c5b-9520-b457879b8bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY4ymVII9JJv"
   },
   "source": [
    "Move to \"_modified-EfficientDet\" folder and install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFJbk-nr8vqS",
    "outputId": "3eb70dab-c283-4ede-f7aa-90b227dbed32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI (from -r requirements.txt (line 14))\n",
      "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-76iagem1\n",
      "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-76iagem1\n",
      "Collecting lxml>=4.6.1\n",
      "  Downloading lxml-4.7.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.2.2)\n",
      "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.19.5)\n",
      "Requirement already satisfied: Pillow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (7.1.2)\n",
      "Collecting PyYAML>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 82.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.15.0)\n",
      "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.7.0)\n",
      "Collecting tensorflow-addons>=0.12\n",
      "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 70.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.11 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.12.0)\n",
      "Collecting neural-structured-learning>=1.3.1\n",
      "  Downloading neural_structured_learning-1.3.1-py2.py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 85.1 MB/s \n",
      "\u001b[?25hCollecting tensorflow-model-optimization>=0.5\n",
      "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
      "\u001b[K     |████████████████████████████████| 213 kB 71.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.29.26)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0->-r requirements.txt (line 14)) (57.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->-r requirements.txt (line 3)) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.7.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.10.0.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.13.3)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.23.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (13.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.17.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.43.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.12->-r requirements.txt (line 9)) (2.7.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neural-structured-learning>=1.3.1->-r requirements.txt (line 11)) (1.4.1)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from neural-structured-learning>=1.3.1->-r requirements.txt (line 11)) (21.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.5->-r requirements.txt (line 12)) (0.1.6)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.35.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (4.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.4.0->-r requirements.txt (line 8)) (3.1.1)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=263954 sha256=81cfa22ee1973f6bfd63b6e4782d3365af1fe7775fe69872c94cbb49e9bee5c5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cxodc27n/wheels/e2/6b/1d/344ac773c7495ea0b85eb228bc66daec7400a143a92d36b7b1\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: tensorflow-model-optimization, tensorflow-addons, PyYAML, pycocotools, neural-structured-learning, lxml\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: pycocotools\n",
      "    Found existing installation: pycocotools 2.0.4\n",
      "    Uninstalling pycocotools-2.0.4:\n",
      "      Successfully uninstalled pycocotools-2.0.4\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 4.2.6\n",
      "    Uninstalling lxml-4.2.6:\n",
      "      Successfully uninstalled lxml-4.2.6\n",
      "Successfully installed PyYAML-6.0 lxml-4.7.1 neural-structured-learning-1.3.1 pycocotools-2.0 tensorflow-addons-0.15.0 tensorflow-model-optimization-0.7.0\n",
      "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
      "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-vuyqkkdh\n",
      "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-vuyqkkdh\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (57.4.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (0.29.26)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('drive/MyDrive/_modified-EfficientDet')\n",
    "sys.path.append('.')\n",
    "!pip install -r requirements.txt\n",
    "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-A0FGxWW_Xj_",
    "outputId": "f8103aaf-eecf-4d0f-edb6-8fa508ba77b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-02 08:30:15--  https://user-images.githubusercontent.com/11736571/77320690-099af300-6d37-11ea-9d86-24f14dc2d540.png\n",
      "Resolving user-images.githubusercontent.com (user-images.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to user-images.githubusercontent.com (user-images.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4080549 (3.9M) [image/png]\n",
      "Saving to: ‘img.png’\n",
      "\n",
      "img.png             100%[===================>]   3.89M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2022-02-02 08:30:15 (41.8 MB/s) - ‘img.png’ saved [4080549/4080549]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'efficientdet-d0_1-5'  #@param\n",
    "\n",
    "\n",
    "# Prepare image and visualization settings.\n",
    "image_url =  'https://user-images.githubusercontent.com/11736571/77320690-099af300-6d37-11ea-9d86-24f14dc2d540.png'#@param\n",
    "image_name = 'img.png' #@param\n",
    "!wget {image_url} -O img.png\n",
    "import os\n",
    "img_path = os.path.join(os.getcwd(), 'img.png')\n",
    "\n",
    "min_score_thresh = 0.35  #@param\n",
    "max_boxes_to_draw = 200  #@param\n",
    "line_thickness =   2#@param\n",
    "\n",
    "import PIL\n",
    "# Get the largest of height/width and round to 128.\n",
    "image_size = max(PIL.Image.open(img_path).size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8iBnul8i9Aq"
   },
   "source": [
    "Visualize model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufiERLoDi6IP"
   },
   "outputs": [],
   "source": [
    "!python model_inspect.py --model_name={MODEL} --logdir=logs &> /dev/null\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kNJiErEw8wC",
    "outputId": "2757a4bf-6c49-445f-8ca1-9f5849249e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_per_epoch = 1200\n"
     ]
    }
   ],
   "source": [
    "# Get Trash-ICRA19 trainval data (TFrecord previously generated)\n",
    "\n",
    "file_pattern_train = 'TrashICRA19-train-*-of-19.tfrecord'\n",
    "\n",
    "images_per_epoch = 1200  #Only the first 4 shards are used as they have annotations for row, plastic and bio only (the other shards add other classes). Each shard has 300 samples.   \n",
    "\n",
    "print('images_per_epoch = {}'.format(images_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5UyF8xwKDuL"
   },
   "outputs": [],
   "source": [
    "# Train efficientdet from scratch with backbone checkpoint.\n",
    "backbone_name = {\n",
    "    'efficientdet-d0': 'efficientnet-b0',\n",
    "    'efficientdet-d0_1-5': 'efficientnet-b0', #BiFPN with depth 1, box/class nets with depth 5\n",
    "    'efficientdet-d0_5-1': 'efficientnet-b0', #BiFPN with depth 5, box/class nets with depth 1\n",
    "    'efficientdet-d1': 'efficientnet-b1',\n",
    "    'efficientdet-d1_1-5': 'efficientnet-b1',\n",
    "    'efficientdet-d2': 'efficientnet-b2',\n",
    "    'efficientdet-d2_1-5': 'efficientnet-b2',\n",
    "    'efficientdet-d3': 'efficientnet-b3',\n",
    "    'efficientdet-d3_1-5': 'efficientnet-b3',\n",
    "    'efficientdet-d4': 'efficientnet-b4',\n",
    "    'efficientdet-d4_1-5': 'efficientnet-b4',\n",
    "    'efficientdet-d5': 'efficientnet-b5',\n",
    "    'efficientdet-d6': 'efficientnet-b6',\n",
    "    'efficientdet-d7': 'efficientnet-b6',\n",
    "}[MODEL]\n",
    "\n",
    "\n",
    "# generating train tfrecord is large, so we skip the execution here.\n",
    "import os\n",
    "if backbone_name not in os.listdir():\n",
    "  !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/ckptsaug/{backbone_name}.tar.gz\n",
    "  !tar xf {backbone_name}.tar.gz\n",
    "\n",
    "!mkdir tmp_model_dir\n",
    "# key option: use --backbone_ckpt rather than --ckpt.\n",
    "# Don't use ema since we only train a few steps.\n",
    "!python main.py --mode=train_and_eval \\\n",
    "    --train_file_pattern=tfrecord/{file_pattern_train} \\\n",
    "    --val_file_pattern=tfrecord/{file_pattern_train} \\\n",
    "    --model_name={MODEL} \\\n",
    "    --model_dir=tmp_model_dir/{MODEL}-scratch-Trash  \\\n",
    "    --backbone_ckpt={backbone_name} \\\n",
    "    --train_batch_size=4 \\\n",
    "    --eval_batch_size=4 --eval_samples={images_per_epoch}  \\\n",
    "    --num_examples_per_epoch={images_per_epoch}  --num_epochs=35  \\\n",
    "    --hparams=\"num_classes=3,moving_average_decay=0,mixed_precision=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0lUxpFYdUB-"
   },
   "source": [
    "Visualize the performance and training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jW5AjDQQiJv"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tmp_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka6_R4zqZD_k"
   },
   "source": [
    "Evaluate the network latency, i.e. from first convolution to box/class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7qA8Rx_aCcd"
   },
   "outputs": [],
   "source": [
    "print(MODEL)\n",
    "!python model_inspect.py --runmode=bm --model_name={MODEL} --hparams=\"mixed_precision=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLmopF0LbSQx"
   },
   "source": [
    "REFERENCES\n",
    "\n",
    "[1] F. Zocco et al., Towards More Efficient EfficientDets and Low-Light Real-Time Marine Debris Detection, https://arxiv.org/pdf/2203.07155.pdf\n",
    "\n",
    "[2] https://github.com/google/automl/blob/master/efficientdet/tutorial.ipynb\n",
    "\n",
    "[3] https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Tutorial_training_TrashICRA19_federico_zocco.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
